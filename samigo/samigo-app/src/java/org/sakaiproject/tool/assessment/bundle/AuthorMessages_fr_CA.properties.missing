Missing fr_CA translations

button_add_questions=Add questions
button_search_questions_by_tag_and=Search questions matching ALL tags
button_search_questions_by_tag_or=Search questions matching AT LEAST one tag
button_search_questions_by_text_and=Search questions matching ALL the words
button_search_questions_by_text_or=Search questions matching  AT LEAST one of the words
search_question_by_text_tip=You can use use quotes to group several words to find only exact matches
origin = Origin
origins = Places that use this question:
preview_tip = The preview button will show you additional information about the question. The questions can be duplicated in several places (question pools and assessments). The result list only shows one of these instances of the question. The Origin field shows where this specific result comes from. You can view all the places where this question is used in the preview.
preview = Preview
loading = Question loading...
calc_question_var_and_formula_label=Variables and formulas
invalid_min_scale=Min decimal places greater than scale value
invalid_max_scale=Max decimal places greater than scale value
feedback_unavailable=Feedback not available for this selection
all_im_descriptions_needed= You must set all descriptions
description_asi_aut=Description: ASI Author XML Style Sheet: Multimedia Audio
image_map_alt=Provide an alternative text for the uploaded image
lead_in_statement=Lead-In
mathjax_usage_warning=LaTeX markup may conflict with notation required for this question type.
question_answer_combinations=Item Stems
random_draw_questions_prefix=* Draw
search_question=Search questions
label_question=Q{0}
label_question_part=P{1}-Q{0}
label_question_part_pool=P{1}-Q{0}-Pool:{2}
label_question_pool=Q{0}-Pool:{2}
edit_published_assessment_warn_edit_pool_questions=Your changes will affect this assessment only, not the question pool from which these questions were drawn. To edit the corresponding question in the question pool named {0}, use the Question Pools link at the top of this page.
what_is_republish_3=if you have modified content that has no bearing on an assessment's overall score (e.g., wording of a question or answer choice).
what_is_regrade_republish_12=NOTE: If you had made manual adjustments to auto-scored questions prior to editing a published assessment, clicking Regrade and Republish will override these manually adjusted scores. However, manually graded questions (e.g., Short Answer/Essay, Audio Recording, File Upload) will not be touched.
case_sensitive_example=Example: if the correct answer is "ABC" and a student's response is "aBc", then the response would be marked as incorrect. 
mutually_exclusive=<b>Mutually exclusive?</b>
t_removeO=Remove Option
t_removeI=Remove Item
note_negative_point_value_question=Optional. Pertains only to 'True False' or 'Multiple Choice, Single Correct' questions.
actions_for=Actions for:
reset_grading_logic=Reset to Default Grading Logic
enable_nagative_marking=Enable Negative Marking
percentage_value=% Value
reset_score_values=Reset Score Values
enter_new_pc_value=Enter new value (optional)
answer_point_value_error=A valid answer point value has not been entered.
theme_text_error=Theme text has not been entered.
simple_text_options_blank_error=Some simple text options are left blank - Please enter option text or remove options before saving.
number_of_rich_text_options_error=Number of rich text options not selected.
missing_or_invalid_answer_options_labels_error=Missing or invalid answer option labels
rich_text_options_error=Rich text answer options text or attachment required
at_least_two_options_required_error=At least 2 options required
at_least_two_pasted_options_required_error=At least 2 pasted options required
blank_or_non_integer_item_sequence_error=Blank or non-integer item sequence encountered for item:
correct_option_labels_error=Correct option labels blank or contain other than alpha characters and commas for item:
correct_option_labels_invalid_error=Invalid option in correct option labels for item:
item_text_not_entered_error=Item text or attachment not entered for item:
simple_text_option_label=Simple text - for a list of items with no formatting
simple_text_paste_label=Simple text - for pasting options
rich_text_option_label=Rich text / Attachments - for styled text, tables, labelled images
please_select_from_available=Question {1} has an invalid answer, {0}. Only the following letters of the alphabet are acceptable: {3}
duplicate_responses=Duplicate responses:
mcsc_whats_this_text=This is the default option because it's the most common: students get the full point value for selecting a single correct answer and zero points for selecting anything else. This option can be further modified by selecting Enable Partial Credit or Enable Negative Marking.
mcms_whats_this_partial_text=<b>Correct minus Incorrect:</b>  They will be given partial credit for selecting a correct answer and will be penalized for selecting an incorrect answer. For instance, question X is worth 10 points and has options A, B, C, D, E where A and B are correct choices. Each correct answer has a point value equal to the question point value divided by the number of correct answers. Thus, a student will get 5 points for selecting A or B. They will be penalized 5 points for selecting C, D, or E.
mcms_whats_this_partial_note=<b>Note: </b> Even though a question could have a negative point value based on the grading logic (as in this example), the actual lowest possible total point value for a multiple correct, multiple selection question cannot be less than zero.
enable_partial_credit_text=Select this option if you want to give students partial credit when they choose an answer that is "almost correct." For instance, question X has options A, B, C, D, where A is the correct answer. If students pick A, they would receive full credit, but they could be given some credit for choosing an answer that is almost correct. Enabling this option will allow you to specify a percentage value for each answer.
enable_negative_makrinkg_text=Select this option if you want to discourage random guessing. The value entered into the "Negative point value..." field will be the question's point value if a student makes an incorrect selection. For instance, if a question is worth 10 points and you enter "5" into the "Negative point value..." field, a correct answer selection would be worth 10 points and an incorrect answer selection would be worth -5 points.  
enable_negative_makrinkg_note=<b>NOTE:</b> Even though a question may have a negative point value, the final point value for an assessment will not be less than zero.
reset_to_default_grading_logic_text=Click this link if you change your mind after selecting 'Enable Partial Credit' or 'Enable Negative Marking'. The default grading logic grants full credit for selecting a correct answer and zero credit for selecting an incorrect answer.
image_map_default_instruction=Identify the item(s) listed below by clicking on the corresponding part of the image. The green button indicates the item you are working on.
addColumnChoiceToFavor=Add Column Choices to Favorites
favorLabel=Favorites Label
commentField=Comment
selectFromFavorites=Select from Favorites
emi_pt=Item Points
emi_whats_this=?
emi_whats_answer_point_title=Answer Point
emi_whats_answer_point=Points are given per item. The overall point score for the question is automatically calculated from the sum of the item points.
emi_whats_theme_title=Theme
emi_whats_theme=The theme describes the main topic/domain that the item set will address, and is linked to the learning objectives.  A more generic theme (such as "Diarrhoea") may be used for sampling knowledge over wider fields, and items in that set may include pharmacology, epidemiology and investigations. A more focussed theme (such as "Management of diarrhoea") is used to sample in-depth knowledge, in this case all related to pharmacology.
emi_whats_options_title=Options
emi_whats_options=The options list consists of the potential answer choices to the item, and includes the distracters that are the incorrect or less likely answers for this particular item. The options should be homogenous, or "the same kind of thing" such as drugs, organisms, cells or investigations, and should be listed only as words or short phrases. Options that are unnecessarily complicated (combinations of options) or "all/none of the above" are not allowed.
emi_whats_leadin_title=Lead-In
emi_whats_leadin=The lead-in statement specifies the association between the stem and the option list. In the R-type format, the options list (for example, 8 or more potential diagnoses) is summarized in the lead-in statement, for example, "For each of the following patients presenting to the Emergency Room, select the most likely diagnosis." The lead-in should provide clear instructions how the student should respond to the item set.
emi_whats_items_title=Item Stems
emi_whats_items=The stem or item describes the case and what is required from the student. It usually consists of a clinical scenario and a question, for example "A 30yr old man presents with... Select the most likely diagnoses." In the R-type format, the question may be included in the stem or may be situated in the lead-in statement.
edit_fib_warning=Please note that adding, removing or modifying the blanks can lead to scoring discrepancies when existing submissions are automatically regraded. For editing this type of question, you should allow resubmission.
edit_order_warning=You may have assigned the same destination position to several parts or questions. The result of the ordering has been modified.
tag_title=Tags
tag_previous=Tags added previously to the question
tag_question_new=Add new tags to the question
tag_question=Add/remove tags to the question
tag_search=Tag Search
text_search=Text Search
results_text= Results for Text search
results_tag= Results for Tag search
tag_text_error=You need at least 3 characters in the text field. Note that HTML (or similar tags) and some special chars are filtered from the text to search.
tag_text_error2=The text has something not allowed by the search engine, usually it is related with unclosed quotation marks. Please, review it
tag_text_error3=An error happened searching, please try to use a more simple query. If the error continues, please contact with your administrator
tag_tags_error=You need at least 1 tag to perform the search.
tag_multitag_singlequestion = Selecting this option, ALL the questions completely identical to this one, (used in other assessments, or question pools), will be updated with these tags.
tag_multitag_singlequestion_all = The system has been configured to copy these tags to any identical question in the system (maybe copied to another test or another question pool).
question_use_rubric=This question could be graded using a Rubric

Missing from new version, present in old fr_CA

